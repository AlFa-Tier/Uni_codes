{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Rechnernutzung in der Physik\n",
    "**Institut für Experimentelle Teilchenphysik**  \n",
    "Prof. G. Quast, Dr. Th. Chwalek  \n",
    "WS 2024/25 – Blatt 05\n",
    "\n",
    "Abgabe: Mo./Di. 13./14. Januar bzw. Mo./Di. 20./21. Januar  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vom 16.12.2024 bis 22.01.2025 läuft die Evaluierung dieser Vorlesung und der zugehörigen Übung. Bitte, nehmen Sie sich kurz Zeit und bewerten Sie Vorlesung und Übung. Die Umfrage finden Sie unter folgenden Links: \n",
    "\n",
    "[Link zur Vorlesungs-Umfrage](https://onlineumfrage.kit.edu/evasys/online.php?p=NL6Q6) \n",
    "\n",
    "[Link zur Übungs-Umfrage](https://onlineumfrage.kit.edu/evasys/online.php?p=RMLHT) \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf dem fünften Übungsblatt beschäftigen Sie sich vertieft mit der Parameterschätzung und wenden einen Hypothesentest an, um die Gemeinsamkeit zweier Datensätze zu überprüfen. \n",
    "\n",
    "Auf dem vergangenen Übungsblatt haben Sie die Parameterschätzung selbst implementiert, indem Sie eine Kostenfunktion (*Likelihood*) geschrieben, diese an diskreten Stellen eines vorgegebenen Parameterraumes ausgewertet und das Minimum an einer der Stützstellen berechnet haben. Wie Sie aus der Vorlesung oder Ihrer eigenen Erfahrung kennen, weist das Vorgehen an vielen Stellen Optimierungsbedarf auf. Sie könnten viel mehr Stützstellen verwenden, um einen besseren Scan der Likelihood zu erhalten, dann müssen Sie aber viel mehr rechnen, zudem müssen Sie Unsicherheiten berücksichtigen, das Verfahren für alle zukünftigen Herausforderungen verallgemeinern und alles so niederschreiben, dass der Code lesbar und einfach verständlich ist. An dieser Stelle können wir Sie beruhigen, denn all diese Aufgaben haben bereits eine Vielzahl an Menschen für Sie erledigt.\n",
    "\n",
    "Im Folgenden beschäftigen Sie sich mit einem Minimierungsverfahren, welches Ihnen erlaubt deutlich effizienter ein Minimum in einer Parameterlandschaft zu finden.\n",
    "Als weitere kleine Aufgabe sollen Sie eine der klassischen Profile-Likelihood-Kurven\n",
    "analysieren, die Vorhersage der Masse des Higgs-Bosons aus Anpassungen von \n",
    "Rechnungen im Rahmen des Standard-Modells der Teilchenpysik and Präzisionsmessungen.\n",
    "Zum Abschluss wenden sie den *Student'schen-t-Test* an und lernen ein modernes\n",
    "Verfahren zur Durchführung nicht-parametrischer Tests kennen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Aufgabe 1: Simulated Annealing <a id=\"Aufgabe1\"></a>\n",
    "---\n",
    "\n",
    "Wie Sie in Ihrem Studium möglicherweise bereits erkannt haben, ist das Finden eines Optimums einer hochdimensionalen Kostenfunktion $F(\\vec{x})$ eine häufige Fragestellung. Damit Sie eine Vorstellung der Lösungsansätze erhalten, sollen Sie in dieser Aufgabe selbst einen Algorithmus schreiben, der Ihnen dabei hilft, jenes Optimum zu bestimmen. Der Algorithmus ist bei weitem nicht der beste oder schnellste, verdeutlich Ihnen aber den Einsatz von Monte-Carlo-Methoden in der Modernen Physik.<br>\n",
    "Als Beispiel betrachten Sie die zweidimensionale, modifizierte Rosenbrock-Funktion\n",
    "\n",
    "$$\n",
    "F(x,y)=\\left(x^2+y-a\\right)^2+\\left(x+y^2-b\\right)^2+c\\cdot\\left(x+y\\right)^2\n",
    "$$\n",
    "\n",
    "mit drei Parametern $a$, $b$ und $c$. Die Funktion weist einige lokale Minima, aber immer nur ein globales Minimum auf. Die Bearbeitung der Aufgabe erfolgt für die zufällige Wahl an Startparametern: $a=11$, $b=7$, $c=0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Implementierung des Algorithmus\n",
    "In der ersten Teilaufgabe erstellen Sie das Grundgerüst der Minimierung, indem Sie zunächst die modifizierte Rosenbrock-Funktion und anschließend das simulierte Abkühlen als Funktionen definieren. Zur Regulierung der Temperatur wird eine konstante Kühlrate verwendet, die als Parameter des Algorithmus betrachtet wird. Zusätzlich wird eine Schrittweite als Parameter eingeführt, welche die Distanz eines neuen Zustandes zum alten beeinflusst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# initialize random number generator\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ->> to do: Implementierung der modifizierten Rosenbrock-Funktion mit der Signatur\n",
    "#            modified_rosenbrock_function(x, par)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ->> to do: Implementierung des Algorithmus für simulated annealing\n",
    "#  Signatur der Funktion \n",
    "#   simulated_annealing(init_vals=[0,0], rosenbrock_pars=[0, 0, 0], init_temp=100, \n",
    "#                       final_temp=1, cool_speed=1, step_size=1)\n",
    "\n",
    "    \"\"\"\n",
    "    Minimize the modified Rosenbrock function using simulated annealing.\n",
    "    \n",
    "    params:\n",
    "        init_vals:       Initial x and y values.\n",
    "        rosenbrock_pars: Parameters of the modified Rosenbrock function.\n",
    "        init_temp:       Initial temperature the cooling starts from.\n",
    "        final_temp:      Final temperature of the cooling.\n",
    "        cool_speed:      Cooling speed in percent of the current temperature.\n",
    "        step_size:       Step size used in the cooling procedure.\n",
    "        \n",
    "    returns:\n",
    "        min_pars: List of floats.\n",
    "                    List of the x and y values at the found minimum.\n",
    "        listOfPoints: List of floats.\n",
    "                    List of the visited points during the minimization process.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Finden des globalen Minimums\n",
    "Testen Sie zunächst Ihren Algorithmus aus, indem Sie Startwerte für $x$ und $y$ in der Nähe des globalen Minimums wählen: $(x,y)=(-2, 3)$. Machen Sie sich dabei mit den Eigenschaften der Anfangs- und Endtemperaturen, der Abkühlrate und der Schrittgröße vertraut.\n",
    "\n",
    "Sobald Sie mit Ihrem Algorithmus zufrieden sind, dieser also zuverlässig gegen das globale Minimum konvergiert, wählen Sie als Startpunkt einen Punkt in einem der Nebenminima (z. B. $(x,y)=(3, -2)$) aus und passen Sie die Parameter des Algorithmus so an, dass er wieder zuverlässig gegen das globale Minimum konvergiert. <br>\n",
    "Zur korrekten Einstellung der Parameter bietet es sich an, entweder einen Bereich an Parametern systematisch zu untersuchen oder den Einfluss eines jeden Parameters graphisch zu untersuchen. Sie finden einen Vorschlag zur graphischen Untersuchung in den folgenden Zellen. In beiden Fällen bietet es sich jedoch an, sich die Eigenschaften der Algorithmusparameter vorher zu überlegen.\n",
    "\n",
    "#### Hinweise und Überlegungen:\n",
    "* Die Anfangs- und Endtemperatur sollen die Kostenfunktionswerte der Anfangsbedingung und des gesuchten Minimums widerspiegeln.\n",
    "* Die Differenz zwischen Anfangs- und Endtemperatur und die Kühlrate beeinflussen die Anzahl an Iterationen.\n",
    "* Die Temperaturskala beeinflusst die Wahrscheinlichkeit für Sprünge.\n",
    "* Die Schrittgröße sollte adäquat im Bezug zum Abstand der Minima gewählt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ->> to do: Tunen und Testen des oben implementierten Algorithmus\n",
    "#     Aufgabe: Konvergenz für den Startpunkt (a = 11, b = 7, c = 0.1) zeigen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional** : grafische Darstellung verchiedener Markov-Ketten, evtl. auch als Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation\n",
    "from IPython.display import Image \n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset matplotlib figures\n",
    "plt.clf()\n",
    "#%matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Aufgabe 2: Interpretation einer Profile-Likeliood-Kurve<a id=\"Aufgabe2\"></a>\n",
    "---\n",
    "\n",
    "In dieser kleinen Aufgabe sollen Sie eine berühmte Profile-Likelihood-Kurve auswerten, \n",
    "die in der Grafik unten gezeigt ist. Die Kurve stellt das Ergebnis einer $\\chi^2$-Anpassung\n",
    "von Rechnungen im Standard-Modell der Teilchenphysik an Präzisionsmessungen dar. Die damals\n",
    "(im Frühjahr 2012) noch unbekannte Masse des Higgs-Bosons ist ein freier Parameter in\n",
    "dieser Anpassung, für den die unten gezeigte Profile-Likelihood bestimmt wurde. \n",
    "Bitte beachten, dass gilt $\\Delta \\chi^2 \\,=\\, 2\\Delta\\log\\cal{L}_p$.\n",
    "\n",
    "Die Profile-Likelihood der Higgs-Masse ist in der folgenden Code-Zelle gezeigt;\n",
    "es handelt sich dabei um die Original-Grafik, der ein Koordinatensystem überlagert wurde. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle ausführen\n",
    "\n",
    "%matplotlib widget \n",
    "# - import image library \n",
    "from PIL import Image\n",
    "# - cleaer all figures\n",
    "plt.clf()\n",
    "# - load image \n",
    "image = np.asarray(Image.open('blueband_Higgs2012.png'))\n",
    "# print(repr(image))\n",
    "# - display image in matplotlib Figure\n",
    "imfig = plt.figure(figsize=(8., 8.))\n",
    "ax0 = imfig.subplots()\n",
    "im = ax0.imshow(image)\n",
    "_ = ax0.axis('off')\n",
    "# - overlay 2nd axis\n",
    "ax = imfig.add_axes([0.2642, 0.2435,  0.617, 0.623])\n",
    "ax.set_xlim(40, 200)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim(0,6)\n",
    "ax.patch.set_alpha(0.01)\n",
    "_ = ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe**: Bestimme Sie die Werte für $\\Delta\\chi^2$, die dem $\\pm1\\,\\sigma$-Intervall und\n",
    "der Obergrenze für die Masse des Higgs-Bosons mit 95% Konfidenzniveau entsprechen.\n",
    "\n",
    "Zeichnen Sie die entsprechenden Linien in die Grafik oben ein. Lesen Sie die Werte\n",
    "für das $\\pm1\\,\\sigma$-Intervall und die 95% Obergrenze ab. \n",
    "\n",
    "*Hinweis*:  \n",
    "    Nutzen Sie die Quantile der Gaußverteilung, um zu bestimmen, bei welchem Wert\n",
    "    von $\\Delta\\chi^2$ Sie das 95%-Niveau erreichen!  Nutzen Sie dazu die kumulative\n",
    "    Gaußverteilung im Paket *scipy.stats.norm*, *.cdf()* oder *.ppf()*. Recherchieren\n",
    "    Sie, was die Funktion *norm.ppf()* bedeutet.  \n",
    "*Erinnerung aus der Vorlesung:*  \n",
    "    Ein Wert von $\\Delta\\chi2 = z^2$ oberhalb des Minimums entspricht einem\n",
    "    Konifdenz-Intervall von $ \\pm z\\,\\sigma$ einer Gaußverteilung. Die Größe\n",
    "    $z$ nennt man auch *\"$z$-value*\".  \n",
    "    Für eine einseitige Grenze, also den Bereich $[-\\infty, \\mu + z\\,\\sigma]$ \n",
    "    von 95% könnte man wegen der Symmetrie der Gaußverteilung auch das Quantil \n",
    "    $[\\mu - z\\,\\sigma, \\mu + z\\,\\sigma]$ für 90% berechnen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> to do: Berechen des \\Delta\\chi^2 Werts für ein 95% einseitiges Limit \n",
    "#       Einzeichnen in die Grafik oben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Aufgabe 3: Vergleich von zwei Verteilungen durch *Hypothesentests* <a id=\"Aufgabe3\"></a>\n",
    "---\n",
    "\n",
    "In der letzten Aufgabe machen Sie sich an einem einfachen Beispiel klar, wie ein sehr simpler \n",
    "Hypothesentest Ihnen dabei helfen kann, eine von Ihnen geforderte Entscheidung zu treffen. Als \n",
    "Beispiel dienen zwei Datensätze, *sample1.dat* und *sample2.dat*. Sie sollen die Aussage treffen, \n",
    "ob die Datensätze Strichproben aus einer gemeinsamen Grundgesamtheit sind, oder ob sie\n",
    "unterschiedlich sind. Dazu führt man üblicherweise einen \n",
    "[*Studentschen-t-Test*](https://de.wikipedia.org/wiki/Zweistichproben-t-Test#Welch-Test) durch. \n",
    "Als auf einen  möglichen Unterschied der beiden Proben empfindliche Prüfgröße (entl.: test statistic) \n",
    "verwendet man die Differenz $d$ der Erwartungswerte $\\mu_1$ und $\\mu_2$ normiert auf deren \n",
    "Unsicherheit $\\sigma$: \n",
    "\\begin{align*}\n",
    "d &= \\frac{\\left|\\mu_1-\\mu_2\\right|}{\\sigma}, \\\\\n",
    "\\sigma &= \\sqrt{\\frac{\\sigma_1^2}{N_1}+\\frac{\\sigma_2^2}{N_2}}.\n",
    "\\end{align*}\n",
    "Dabei geben $\\sigma_{1,2}$ bzw. $N_{1,2}$ die Standardabweichung bzw. Größe der Stichproben an.\n",
    "Die zu überprüfende Nullhypothese $H_0$ **Die Stichproben haben gleiche Erwartungswerte** geht\n",
    "davon aus, dass $\\mu_1\\,=\\,\\mu_2$ gilt. \n",
    "\n",
    "Unter der Annahme, dass die Stichproben gaußverteilt sind, ist die Verteilung der Differenzen\n",
    "der Mittelwerte durch die *Student'sche t-Verteilung* geben, die im Paket *scipy.stats.t*\n",
    "bereit gestellt wird. \n",
    "\n",
    "Für den Hypothesentest wird nun die Prüfgröße, $d_\\mathrm{obs}$ der beiden Stichproben\n",
    "berechnet und das Quantil der *t*-Verteilung für $|d| > |d_ \\mathrm{obs}|$ bestimmt. \n",
    "Wenn dieses Quantil kleiner als ein vorher festgelegter kritischer Wert $\\alpha$ ist, \n",
    "wird die Nullhypothese verworfen. Für $\\alpha$ wählt man typischerweise Werte von \n",
    "10%, 5% oder auch 1%, abhängig davon, wie hoch die Kosten für eine Fehleinschätzung \n",
    "im jeweiligen Anwendungsfall ausfallen. Für unser Beispiel wählen wir $\\alpha\\,=\\,0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Vorbereitung des Hypothesentests\n",
    "\n",
    "Implementieren Sie eine Funktion zur Berechnung der Größe $d$ und der Anzahl der Freiheitsgrade\n",
    " $n_\\mathrm{dof}=N_1+N_2-2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> to do:  Funktion zur Berechnung der Student'schen Prüfgröße, Signatur der Funktion\n",
    "#             calc_Student_t(d1,d2):\n",
    "  '''\n",
    "    Calculate test statistic t for Student's t-test\n",
    "    input: d1, d2: 1d numpy-arrays containing the data\n",
    "    Args: \n",
    "     d1, d2: 1d numpy-arrays containing the data\n",
    "   \n",
    "    Returns:\n",
    "     float: value of Student's test\n",
    "     float: number of degrees of freedom\n",
    "  '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe**: Wie immer ist es ratsam, eine grafische Darstellung der Daten zu erzeugen, \n",
    "sich einen Überblick zu verschaffen. Lesen Sie die Daten ein und stellen Sie sie als\n",
    "Histogramme dar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> to do Daten im CSV-Format einlesen und Datensätze grafisch darstellen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) *Studentscher-t-Test*\n",
    "\n",
    "Berechnen Sie nun die beobachtete Größe $d_\\mathrm{obs}$. Stellen Sie die *t*-Verteilung \n",
    "graphisch dar und zeichnen Sie den beobachteten Wert ein. Stellen Sie ebenfalls fest, \n",
    "ob Sie die Nullhypothese mit einer Irrtumswahrscheinlichkeit von $\\alpha=0.05$ verwerfen \n",
    "können.\n",
    "\n",
    "#### Hinweis:\n",
    "- Im *SciPy*-Paket \n",
    "*[scipy.stats.t](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html)* \n",
    "sind Methoden zur Berechnung der Verteilungsdichte (*.pdf()*)\n",
    "  und der kumulativen Verteilungsfunktion (*.cdf()*) implementiert, die Sie im Folgenden \n",
    "  verwenden können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stat\n",
    "import scipy.integrate as integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "# --> to do Berechnen der Prüfgröße, Einzeichnen des Werts der Prüfgröße \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Nicht-parametrischer Test mit der Bootstrap-Methode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Vorgriff auf die nächste Vorlesung soll eine andere, auf dem sogenannten \"Bootstrap\"-Verfahren\n",
    "beruhende Methode (im Deutschen auch \"Münchhausen-Methode\") verwendet, um die Verteilung der\n",
    " Prüfgröße zu bestimmen. Eine analytisch, durch Parameter festgelegte Verteilungsdichte\n",
    "wird dazu nicht benötigt. Die Methode wurde zuerst im Jahr 1979 von B. Efron untersucht.\n",
    "\n",
    "Bootstrapping ist ein Monte Carlo Verfahren, bei dem sehr häufig Stichproben mit Zurücklegen\n",
    "aus der ursprünglichen Stichprobe gewürfelt werden. Das Paket *numpy* ermöglicht das\n",
    "mit der Methode *numpy.choice(data, size)*. \n",
    "\n",
    "In der Code-Zelle unten finden Sie eine Illustration. Aus lediglich 30 Datenpunkten wir durch \n",
    "10'000-faches Resampling (d.h. \"Ziehen mit Zurücklegen\") die Verteilung der Mittelwerte und\n",
    "das zentrale 68,3%-Konfidenzintervall gewonnen! Der Code für das resampling sieht so aus: \n",
    "\n",
    "```\n",
    "# generate multiple samples using Bootstrap method,\n",
    "#      i.e. by resampling form data itself,\n",
    "  niter=10000   # draw <niter> samples\n",
    "  m =  []\n",
    "  for i in range(niter):\n",
    "    mi=np.random.choice(d0, size=n).mean()\n",
    "    m.append(mi) \n",
    "  m = np.array(m)\n",
    "```\n",
    "\n",
    "Hier das vollständig lauffähige Beispiel, das Sie ausprobieren und verstehen sollten, \n",
    "um sich von der Mächtigkeit der Methode zu überzeugen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script bootstrap.py (als Beispiel)\n",
    "''' \n",
    "   Illustrate bootstrap method\n",
    "\n",
    ".. author:: Guenter Quast <g.quast@kit.edu> \n",
    "     fuer den Kurs Rechnernutzung in der Physik\n",
    "'''\n",
    "# -------------------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getData(n=100):\n",
    "  # provide a data sample from uniform distribution\n",
    "  #return 2*np.random.rand(n)-1.\n",
    "  return np.random.randn(n)\n",
    "\n",
    "# Gauss distribution\n",
    "def fGauss(x, mu=0., sigma=1.):\n",
    "    return (np.exp(-(x-mu)**2/2./sigma**2)/np.sqrt(2.*np.pi)/sigma)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "#\n",
    "n=30\n",
    "d0 = getData(n)\n",
    "\n",
    "# caculate classical mean and error on mean\n",
    "mean = d0.mean()\n",
    "std = np.sqrt( d0.var(ddof=1)/n )\n",
    "#  \n",
    "\n",
    "# generate multiple samples using Bootstrap method,\n",
    "#      i.e. by resampling form data itself,\n",
    "niter=10000   # draw <niter> samples\n",
    "m = []\n",
    "for i in range(niter):\n",
    "    mi=np.random.choice(d0, size=n).mean()\n",
    "    m.append(mi) \n",
    "m = np.array(m)\n",
    "\n",
    "# print mean mean\n",
    "print('*==* Test Bootstrap Method:')\n",
    "print( '     mean value -/+ std   %.3f :  %.3f'%(mean - std, mean + std) )\n",
    "print( '*==* Confidence Interval from bootstrap method:')\n",
    "print( '           68%% CI        %.3f : %.3f'\\\n",
    "         %(np.percentile(m, 16 ),np.percentile(m, 84 )) )\n",
    "  \n",
    "\n",
    "# plot data and sample distribution\n",
    "fig0 = plt.figure('Sample from Gaussian Distribution', figsize=(7.5, 5.))\n",
    "ax0 = fig0.add_subplot(1,1,1)\n",
    "x = np.linspace(-3., 3., 200)\n",
    "ax0.plot(x, fGauss(x), 'r-', label='Normalverteilung')   \n",
    "ax0.plot(d0, np.ones(len(d0))*0.005, 'b|', markersize=20, label='Stichprobenwerte')\n",
    "ax0.set_xlabel('x')\n",
    "ax0.set_ylabel('Gauss(x)')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# plot resampled distribution of mean  \n",
    "fig = plt.figure('Bootstrap example', figsize=(7.5, 5.))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "bc, be, _ = ax.hist(m, 100, alpha=0.7)\n",
    "x = np.linspace(-3./np.sqrt(n), 3./np.sqrt(n), 200)\n",
    "ax.plot(x, (be[1]-be[0])*niter*fGauss(x, mu=mean, sigma=1./np.sqrt(n)), 'r--')   \n",
    "ax.set_xlabel('distribution of means')\n",
    "ax.set_ylabel('frequency')\n",
    "ax.axvline(np.percentile(m,16), color='r', linestyle='--')\n",
    "ax.axvline(np.percentile(m,84), color='r', linestyle='--')\n",
    "ax.text(0.45, 0.95, '68% CL', color='darkred',\n",
    "      transform=ax.transAxes)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Bestimmen Sie mit Hilfe der Bootstrap-Methode den p-Wert für die Hypothese, \n",
    "dass beide Verteilungen der gleichen Grundgesamtheit entsprechen. Erzeugen Sie dazu durch \n",
    "10'000-faches Resampling aus der Vereinigungsmenge beider Datensätze jeweils 10'000 \n",
    "Datensätze der Längen $n_1$ und $n_2$ und bestimmen Sie jeweils die Prüfgröße $d_i$. \n",
    "Die Prüfgröße ist die gleiche, die auch schon oben im klassischen t-Test verwendet wurde. \n",
    "\n",
    "Stellen Sie die Verteilung grafisch dar und bestimmen Sie den p-Wert dafür, einen \n",
    "größeren Werd für $d_i$ als den in den Daten tatsächlich beobachteten zu finden. \n",
    "Das ist der p-Wert für das Verwerfen der Null-Hypothese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> to do   Mittels Bootstrapping die Verteilung der Prüfgröße d bestimmen\n",
    "#             p-Wert bzgl. dieser Verteilung bestimmen\n",
    "\n",
    "# read data if not yet done\n",
    "# d1 = np.loadtxt('sample1.dat')\n",
    "# d2 = np.loadtxt('sample2.dat')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
